{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zaryana223/XSID-ru-NLP/blob/main/xSID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machamp"
      ],
      "metadata": {
        "id": "rQMNgIKGn8Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/machamp-nlp/machamp.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brLK_pW0qH61",
        "outputId": "b2342614-617d-4bdf-f3d0-0bd9a4ccb343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'machamp'...\n",
            "remote: Enumerating objects: 1912, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 1912 (delta 93), reused 95 (delta 87), pack-reused 1789\u001b[K\n",
            "Receiving objects: 100% (1912/1912), 1.71 MiB | 5.76 MiB/s, done.\n",
            "Resolving deltas: 100% (1269/1269), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd machamp/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3NywFO58omo",
        "outputId": "44d9a8f7-495d-45b9-bcae-44ce962cd102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/machamp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --user -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL7uhJcL8rUc",
        "outputId": "ed8aad80-6ea7-4eda-931a-5eddf9211a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.40.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.2.1+cu121)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.1.99)\n",
            "Collecting jsonnet (from -r requirements.txt (line 7))\n",
            "  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.66.2)\n",
            "Collecting uniplot (from -r requirements.txt (line 9))\n",
            "  Downloading uniplot-0.12.5-py3-none-any.whl (28 kB)\n",
            "Collecting networkx==2.5 (from -r requirements.txt (line 10))\n",
            "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from networkx==2.5->-r requirements.txt (line 10)) (4.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->-r requirements.txt (line 4)) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->-r requirements.txt (line 4)) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->-r requirements.txt (line 4)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->-r requirements.txt (line 4)) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->-r requirements.txt (line 4)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->-r requirements.txt (line 4)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->-r requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->-r requirements.txt (line 4)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->-r requirements.txt (line 4)) (0.4.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r requirements.txt (line 5)) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r requirements.txt (line 5)) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r requirements.txt (line 5)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r requirements.txt (line 5)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r requirements.txt (line 5)) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->-r requirements.txt (line 5))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->-r requirements.txt (line 5)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.0->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.0->-r requirements.txt (line 4)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.0->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.0->-r requirements.txt (line 4)) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Building wheels for collected packages: jsonnet\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp310-cp310-linux_x86_64.whl size=6406883 sha256=b2e8d63a4d697d3956deba0acc1d5e81c732f759b0d1c5e408cfa43c4c01f291\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/0d/6b/5467dd1db9332ba4bd5cf4153e2870c5f89bb4db473d989cc2\n",
            "Successfully built jsonnet\n",
            "Installing collected packages: jsonnet, uniplot, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed jsonnet-0.20.0 networkx-2.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 uniplot-0.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачиваем данные xSID, кладем в нужную директорию"
      ],
      "metadata": {
        "id": "MMp1_NVeCkwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir data\n",
        "%mkdir data/xSID-0.3/"
      ],
      "metadata": {
        "id": "ZltiOTdZCqSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O data/xSID-0.3/en.train.conll https://raw.githubusercontent.com/mainlp/xsid/main/data/xSID/en.train.conll\n",
        "!wget -O data/xSID-0.3/en.valid.conll https://raw.githubusercontent.com/mainlp/xsid/main/data/xSID/en.valid.conll\n",
        "!wget -O data/xSID-0.3/en.test.conll https://raw.githubusercontent.com/mainlp/xsid/main/data/xSID/en.test.conll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNW_iaTaDF8S",
        "outputId": "7d43177d-71dc-4e41-b2a2-774b4853eb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-03 14:58:10--  https://raw.githubusercontent.com/mainlp/xsid/main/data/xSID/en.train.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14643255 (14M) [text/plain]\n",
            "Saving to: ‘data/xSID-0.3/en.train.conll’\n",
            "\n",
            "data/xSID-0.3/en.tr 100%[===================>]  13.96M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-05-03 14:58:12 (327 MB/s) - ‘data/xSID-0.3/en.train.conll’ saved [14643255/14643255]\n",
            "\n",
            "--2024-05-03 14:58:12--  https://raw.githubusercontent.com/mainlp/xsid/main/data/xSID/en.valid.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 101455 (99K) [text/plain]\n",
            "Saving to: ‘data/xSID-0.3/en.valid.conll’\n",
            "\n",
            "data/xSID-0.3/en.va 100%[===================>]  99.08K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-05-03 14:58:12 (58.0 MB/s) - ‘data/xSID-0.3/en.valid.conll’ saved [101455/101455]\n",
            "\n",
            "--2024-05-03 14:58:12--  https://raw.githubusercontent.com/mainlp/xsid/main/data/xSID/en.test.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 163151 (159K) [text/plain]\n",
            "Saving to: ‘data/xSID-0.3/en.test.conll’\n",
            "\n",
            "data/xSID-0.3/en.te 100%[===================>] 159.33K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-05-03 14:58:12 (55.5 MB/s) - ‘data/xSID-0.3/en.test.conll’ saved [163151/163151]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py --dataset_configs configs/nlu.json --device 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ1QwPUHEjn9",
        "outputId": "9311524c-c886-4ac6-ba18-b380dcc6e5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-03 15:13:48.810503: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-03 15:13:48.810556: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-03 15:13:48.811973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-03 15:13:49.876254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-03 15:13:50,930 - INFO - machamp.model.trainer - cmd: train.py --dataset_configs configs/nlu.json --device 0\n",
            "2024-05-03 15:13:51,000 - INFO - machamp.model.trainer - GPU: Tesla T4\n",
            "2024-05-03 15:13:51,000 - INFO - machamp.model.trainer - Torch version: 2.2.1+cu121\n",
            "2024-05-03 15:13:51,000 - INFO - machamp.model.trainer - Transformers version: 4.40.1\n",
            "2024-05-03 15:13:51,002 - INFO - machamp.model.trainer - MaChAmp git version 9f5a6ce48fcebed353956f28de59d9d99098f073\n",
            "2024-05-03 15:13:51,564 - ERROR - STDERR - \n",
            "2024-05-03 15:13:51,565 - ERROR - STDERR - tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]\n",
            "2024-05-03 15:13:51,565 - ERROR - STDERR - \n",
            "2024-05-03 15:13:51,565 - ERROR - STDERR - tokenizer_config.json: 100%|##########| 49.0/49.0 [00:00<00:00, 242kB/s]\n",
            "2024-05-03 15:13:52,093 - ERROR - STDERR - \n",
            "2024-05-03 15:13:52,093 - ERROR - STDERR - config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]\n",
            "2024-05-03 15:13:52,094 - ERROR - STDERR - \n",
            "2024-05-03 15:13:52,094 - ERROR - STDERR - config.json: 100%|##########| 625/625 [00:00<00:00, 3.26MB/s]\n",
            "2024-05-03 15:13:52,657 - ERROR - STDERR - \n",
            "2024-05-03 15:13:52,658 - ERROR - STDERR - vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]\n",
            "2024-05-03 15:13:52,894 - ERROR - STDERR - \n",
            "2024-05-03 15:13:52,894 - ERROR - STDERR - vocab.txt: 100%|##########| 996k/996k [00:00<00:00, 4.22MB/s]\n",
            "2024-05-03 15:13:52,895 - ERROR - STDERR - \n",
            "2024-05-03 15:13:52,895 - ERROR - STDERR - vocab.txt: 100%|##########| 996k/996k [00:00<00:00, 4.19MB/s]\n",
            "2024-05-03 15:13:53,932 - ERROR - STDERR - \n",
            "2024-05-03 15:13:53,932 - ERROR - STDERR - tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]\n",
            "2024-05-03 15:13:54,394 - ERROR - STDERR - \n",
            "2024-05-03 15:13:54,394 - ERROR - STDERR - tokenizer.json: 100%|##########| 1.96M/1.96M [00:00<00:00, 4.26MB/s]\n",
            "2024-05-03 15:13:54,397 - ERROR - STDERR - \n",
            "2024-05-03 15:13:54,397 - ERROR - STDERR - tokenizer.json: 100%|##########| 1.96M/1.96M [00:00<00:00, 4.23MB/s]\n",
            "2024-05-03 15:13:54,772 - INFO - machamp.data.machamp_dataset - Reading data/xSID-0.3/en.train.conll...\n",
            "2024-05-03 15:14:39,996 - INFO - machamp.readers.read_sequence - Stats NLU (data/xSID-0.3/en.train.conll):\n",
            "2024-05-03 15:14:39,996 - INFO - machamp.readers.read_sequence - Lines:      43,605\n",
            "2024-05-03 15:14:39,996 - INFO - machamp.readers.read_sequence - Words:      341,184\n",
            "2024-05-03 15:14:39,996 - INFO - machamp.readers.read_sequence - Subwords:   433,546\n",
            "2024-05-03 15:14:39,997 - INFO - machamp.readers.read_sequence - Unks:       76\n",
            "2024-05-03 15:14:39,997 - INFO - machamp.readers.read_sequence - Pre-splits: 0\n",
            "2024-05-03 15:14:40,014 - INFO - machamp.data.machamp_dataset - Done reading data/xSID-0.3/en.train.conll (45.0s)\n",
            "\n",
            "2024-05-03 15:14:40,014 - ERROR - STDERR - /usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
            "2024-05-03 15:14:40,015 - ERROR - STDERR -   warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n",
            "2024-05-03 15:14:40,755 - INFO - machamp.data.machamp_dataset - Reading data/xSID-0.3/en.valid.conll...\n",
            "2024-05-03 15:14:41,063 - INFO - machamp.readers.read_sequence - Stats NLU (data/xSID-0.3/en.valid.conll):\n",
            "2024-05-03 15:14:41,063 - INFO - machamp.readers.read_sequence - Lines:      300\n",
            "2024-05-03 15:14:41,063 - INFO - machamp.readers.read_sequence - Words:      2,377\n",
            "2024-05-03 15:14:41,063 - INFO - machamp.readers.read_sequence - Subwords:   3,091\n",
            "2024-05-03 15:14:41,063 - INFO - machamp.readers.read_sequence - Unks:       0\n",
            "2024-05-03 15:14:41,064 - INFO - machamp.readers.read_sequence - Pre-splits: 0\n",
            "2024-05-03 15:14:41,064 - INFO - machamp.data.machamp_dataset - Done reading data/xSID-0.3/en.valid.conll (0.0s)\n",
            "\n",
            "2024-05-03 15:14:42,100 - ERROR - STDERR - \n",
            "2024-05-03 15:14:42,100 - ERROR - STDERR - model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]\n",
            "2024-05-03 15:14:42,235 - ERROR - STDERR - \n",
            "2024-05-03 15:14:42,235 - ERROR - STDERR - model.safetensors:   4%|4         | 31.5M/714M [00:00<00:02, 235MB/s]\n",
            "2024-05-03 15:14:42,341 - ERROR - STDERR - \n",
            "2024-05-03 15:14:42,341 - ERROR - STDERR - model.safetensors:   9%|8         | 62.9M/714M [00:00<00:02, 267MB/s]\n",
            "2024-05-03 15:14:42,462 - ERROR - STDERR - \n",
            "2024-05-03 15:14:42,462 - ERROR - STDERR - model.safetensors:  15%|#4        | 105M/714M [00:00<00:02, 304MB/s]\n",
            "2024-05-03 15:14:42,592 - ERROR - STDERR - \n",
            "2024-05-03 15:14:42,593 - ERROR - STDERR - model.safetensors:  19%|#9        | 136M/714M [00:00<00:02, 278MB/s]\n",
            "2024-05-03 15:14:42,825 - ERROR - STDERR - \n",
            "2024-05-03 15:14:42,835 - ERROR - STDERR - model.safetensors:  23%|##3       | 168M/714M [00:00<00:02, 204MB/s]\n",
            "2024-05-03 15:14:42,979 - ERROR - STDERR - \n",
            "2024-05-03 15:14:42,980 - ERROR - STDERR - model.safetensors:  28%|##7       | 199M/714M [00:00<00:02, 204MB/s]\n",
            "2024-05-03 15:14:43,080 - ERROR - STDERR - \n",
            "2024-05-03 15:14:43,080 - ERROR - STDERR - model.safetensors:  32%|###2      | 231M/714M [00:00<00:02, 230MB/s]\n",
            "2024-05-03 15:14:43,180 - ERROR - STDERR - \n",
            "2024-05-03 15:14:43,180 - ERROR - STDERR - model.safetensors:  37%|###6      | 262M/714M [00:01<00:01, 251MB/s]\n",
            "2024-05-03 15:14:43,356 - ERROR - STDERR - \n",
            "2024-05-03 15:14:43,356 - ERROR - STDERR - model.safetensors:  41%|####1     | 294M/714M [00:01<00:01, 223MB/s]\n",
            "2024-05-03 15:14:43,524 - ERROR - STDERR - \n",
            "2024-05-03 15:14:43,524 - ERROR - STDERR - model.safetensors:  46%|####5     | 325M/714M [00:01<00:01, 211MB/s]\n",
            "2024-05-03 15:14:43,683 - ERROR - STDERR - \n",
            "2024-05-03 15:14:43,685 - ERROR - STDERR - model.safetensors:  50%|####9     | 357M/714M [00:01<00:01, 207MB/s]\n",
            "2024-05-03 15:14:43,838 - ERROR - STDERR - \n",
            "2024-05-03 15:14:43,838 - ERROR - STDERR - model.safetensors:  54%|#####4    | 388M/714M [00:01<00:01, 206MB/s]\n",
            "2024-05-03 15:14:43,988 - ERROR - STDERR - \n",
            "2024-05-03 15:14:43,989 - ERROR - STDERR - model.safetensors:  59%|#####8    | 419M/714M [00:01<00:01, 207MB/s]\n",
            "2024-05-03 15:14:44,134 - ERROR - STDERR - \n",
            "2024-05-03 15:14:44,134 - ERROR - STDERR - model.safetensors:  63%|######3   | 451M/714M [00:02<00:01, 209MB/s]\n",
            "2024-05-03 15:14:44,274 - ERROR - STDERR - \n",
            "2024-05-03 15:14:44,274 - ERROR - STDERR - model.safetensors:  68%|######7   | 482M/714M [00:02<00:01, 214MB/s]\n",
            "2024-05-03 15:14:44,419 - ERROR - STDERR - \n",
            "2024-05-03 15:14:44,419 - ERROR - STDERR - model.safetensors:  72%|#######1  | 514M/714M [00:02<00:00, 215MB/s]\n",
            "2024-05-03 15:14:44,544 - ERROR - STDERR - \n",
            "2024-05-03 15:14:44,544 - ERROR - STDERR - model.safetensors:  76%|#######6  | 545M/714M [00:02<00:00, 225MB/s]\n",
            "2024-05-03 15:14:44,668 - ERROR - STDERR - \n",
            "2024-05-03 15:14:44,668 - ERROR - STDERR - model.safetensors:  81%|########  | 577M/714M [00:02<00:00, 233MB/s]\n",
            "2024-05-03 15:14:44,799 - ERROR - STDERR - \n",
            "2024-05-03 15:14:44,799 - ERROR - STDERR - model.safetensors:  85%|########5 | 608M/714M [00:02<00:00, 235MB/s]\n",
            "2024-05-03 15:14:44,906 - ERROR - STDERR - \n",
            "2024-05-03 15:14:44,907 - ERROR - STDERR - model.safetensors:  90%|########9 | 640M/714M [00:02<00:00, 250MB/s]\n",
            "2024-05-03 15:14:45,019 - ERROR - STDERR - \n",
            "2024-05-03 15:14:45,020 - ERROR - STDERR - model.safetensors:  94%|#########3| 671M/714M [00:02<00:00, 258MB/s]\n",
            "2024-05-03 15:14:45,136 - ERROR - STDERR - \n",
            "2024-05-03 15:14:45,136 - ERROR - STDERR - model.safetensors: 100%|#########9| 713M/714M [00:03<00:00, 287MB/s]\n",
            "2024-05-03 15:14:45,159 - ERROR - STDERR - \n",
            "2024-05-03 15:14:45,159 - ERROR - STDERR - model.safetensors: 100%|##########| 714M/714M [00:03<00:00, 234MB/s]\n",
            "2024-05-03 15:14:46,146 - INFO - machamp.model.machamp - Overview of the torch model: \n",
            "2024-05-03 15:14:46,146 - INFO - machamp.model.machamp - MachampModel(\n",
            "  (mlm): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (decoders): ModuleDict(\n",
            "    (intent): MachampClassificationDecoder(\n",
            "      (hidden_to_label): Linear(in_features=768, out_features=19, bias=True)\n",
            "      (loss_function): CrossEntropyLoss()\n",
            "      (decoder_dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (slots): MachampSeqDecoder(\n",
            "      (hidden_to_label): Linear(in_features=768, out_features=73, bias=True)\n",
            "      (loss_function): CrossEntropyLoss()\n",
            "      (decoder_dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (scalars): ModuleDict(\n",
            "    (intent): None\n",
            "    (slots): None\n",
            "  )\n",
            "  (dataset_embedder): Embedding(1, 768)\n",
            ")\n",
            "2024-05-03 15:14:46,196 - INFO - machamp.utils.myutils - Done constructing parameter groups.\n",
            "2024-05-03 15:14:46,196 - INFO - machamp.utils.myutils - Group 0: ['mlm.encoder.layer.8.output.dense.weight', 'mlm.encoder.layer.2.intermediate.dense.weight', 'mlm.encoder.layer.11.attention.self.key.weight', 'mlm.encoder.layer.1.output.dense.bias', 'mlm.encoder.layer.6.attention.output.dense.weight', 'mlm.encoder.layer.7.attention.output.LayerNorm.weight', 'mlm.encoder.layer.9.attention.output.dense.bias', 'mlm.encoder.layer.8.attention.self.key.weight', 'mlm.encoder.layer.8.intermediate.dense.weight', 'mlm.encoder.layer.5.intermediate.dense.bias', 'mlm.encoder.layer.8.attention.output.LayerNorm.bias', 'mlm.encoder.layer.3.output.dense.weight', 'mlm.encoder.layer.7.attention.output.LayerNorm.bias', 'mlm.encoder.layer.8.attention.self.query.bias', 'mlm.encoder.layer.6.intermediate.dense.bias', 'mlm.encoder.layer.3.attention.output.dense.weight', 'mlm.encoder.layer.6.attention.output.LayerNorm.bias', 'mlm.encoder.layer.9.intermediate.dense.bias', 'mlm.encoder.layer.0.output.dense.bias', 'mlm.encoder.layer.7.attention.output.dense.bias', 'mlm.pooler.dense.bias', 'mlm.encoder.layer.4.output.LayerNorm.bias', 'mlm.encoder.layer.5.attention.self.value.weight', 'mlm.encoder.layer.7.attention.self.query.bias', 'mlm.encoder.layer.0.output.LayerNorm.weight', 'mlm.encoder.layer.3.output.LayerNorm.bias', 'mlm.encoder.layer.2.attention.output.dense.bias', 'mlm.encoder.layer.10.output.LayerNorm.bias', 'mlm.encoder.layer.3.output.dense.bias', 'mlm.encoder.layer.11.attention.self.key.bias', 'mlm.encoder.layer.1.attention.output.dense.weight', 'mlm.encoder.layer.3.attention.self.value.bias', 'mlm.encoder.layer.4.output.LayerNorm.weight', 'mlm.encoder.layer.1.output.dense.weight', 'mlm.encoder.layer.6.attention.self.query.bias', 'mlm.encoder.layer.2.attention.self.query.bias', 'mlm.encoder.layer.6.attention.self.value.bias', 'mlm.encoder.layer.10.attention.output.dense.bias', 'mlm.encoder.layer.0.attention.self.value.bias', 'mlm.encoder.layer.3.attention.output.dense.bias', 'mlm.encoder.layer.3.intermediate.dense.bias', 'mlm.encoder.layer.10.attention.self.query.bias', 'mlm.encoder.layer.10.attention.self.value.bias', 'mlm.encoder.layer.9.attention.self.value.bias', 'mlm.encoder.layer.11.output.dense.bias', 'mlm.encoder.layer.3.attention.output.LayerNorm.bias', 'mlm.encoder.layer.9.output.LayerNorm.bias', 'mlm.encoder.layer.1.attention.output.LayerNorm.weight', 'mlm.encoder.layer.10.attention.output.LayerNorm.weight', 'mlm.encoder.layer.2.attention.self.value.weight', 'mlm.encoder.layer.11.output.LayerNorm.bias', 'mlm.encoder.layer.4.attention.output.LayerNorm.bias', 'mlm.encoder.layer.2.attention.self.key.weight', 'mlm.encoder.layer.5.attention.self.query.weight', 'mlm.encoder.layer.5.attention.output.dense.weight', 'mlm.encoder.layer.8.intermediate.dense.bias', 'mlm.encoder.layer.3.attention.self.key.weight', 'mlm.encoder.layer.10.attention.output.LayerNorm.bias', 'mlm.encoder.layer.0.attention.output.dense.weight', 'mlm.encoder.layer.5.output.LayerNorm.weight', 'mlm.encoder.layer.1.attention.self.value.weight', 'mlm.encoder.layer.0.attention.self.value.weight', 'mlm.encoder.layer.5.attention.self.value.bias', 'mlm.encoder.layer.7.intermediate.dense.weight', 'mlm.encoder.layer.8.attention.self.value.bias', 'mlm.encoder.layer.4.intermediate.dense.weight', 'mlm.encoder.layer.7.output.LayerNorm.weight', 'mlm.encoder.layer.9.attention.self.query.weight', 'mlm.encoder.layer.1.attention.self.key.bias', 'mlm.encoder.layer.6.output.dense.weight', 'mlm.encoder.layer.8.attention.output.dense.bias', 'mlm.encoder.layer.6.intermediate.dense.weight', 'mlm.encoder.layer.7.intermediate.dense.bias', 'mlm.encoder.layer.5.output.dense.weight', 'mlm.encoder.layer.10.output.dense.weight', 'mlm.embeddings.position_embeddings.weight', 'mlm.encoder.layer.5.attention.self.query.bias', 'mlm.encoder.layer.9.attention.output.LayerNorm.bias', 'mlm.encoder.layer.2.attention.self.key.bias', 'mlm.encoder.layer.1.output.LayerNorm.weight', 'mlm.embeddings.word_embeddings.weight', 'mlm.encoder.layer.3.attention.output.LayerNorm.weight', 'mlm.encoder.layer.11.attention.self.value.bias', 'mlm.encoder.layer.11.attention.output.LayerNorm.bias', 'mlm.encoder.layer.11.attention.self.query.weight', 'mlm.encoder.layer.10.attention.output.dense.weight', 'mlm.encoder.layer.5.attention.output.LayerNorm.weight', 'mlm.encoder.layer.1.attention.self.key.weight', 'mlm.encoder.layer.2.attention.output.LayerNorm.weight', 'mlm.encoder.layer.11.intermediate.dense.bias', 'mlm.encoder.layer.4.attention.self.key.bias', 'mlm.encoder.layer.11.attention.self.value.weight', 'mlm.encoder.layer.1.attention.output.LayerNorm.bias', 'mlm.encoder.layer.5.intermediate.dense.weight', 'mlm.encoder.layer.11.attention.output.dense.weight', 'mlm.encoder.layer.2.output.dense.weight', 'mlm.encoder.layer.8.attention.output.dense.weight', 'mlm.encoder.layer.9.attention.self.key.bias', 'mlm.encoder.layer.11.attention.self.query.bias', 'mlm.encoder.layer.6.attention.output.dense.bias', 'mlm.encoder.layer.4.attention.self.value.bias', 'mlm.encoder.layer.0.attention.self.query.weight', 'mlm.encoder.layer.4.output.dense.bias', 'mlm.encoder.layer.2.attention.output.dense.weight', 'mlm.encoder.layer.7.attention.self.value.weight', 'mlm.encoder.layer.3.attention.self.value.weight', 'mlm.encoder.layer.1.output.LayerNorm.bias', 'mlm.encoder.layer.11.output.dense.weight', 'mlm.encoder.layer.7.attention.self.key.bias', 'mlm.encoder.layer.4.attention.output.dense.bias', 'mlm.encoder.layer.0.output.LayerNorm.bias', 'mlm.encoder.layer.6.output.LayerNorm.weight', 'mlm.encoder.layer.4.attention.self.key.weight', 'mlm.encoder.layer.7.attention.output.dense.weight', 'mlm.encoder.layer.4.intermediate.dense.bias', 'mlm.encoder.layer.10.attention.self.key.weight', 'mlm.encoder.layer.11.attention.output.dense.bias', 'mlm.encoder.layer.3.attention.self.query.bias', 'mlm.embeddings.LayerNorm.bias', 'mlm.encoder.layer.9.attention.self.key.weight', 'mlm.embeddings.LayerNorm.weight', 'mlm.encoder.layer.4.attention.self.query.bias', 'mlm.encoder.layer.8.output.dense.bias', 'mlm.encoder.layer.9.attention.self.query.bias', 'mlm.encoder.layer.0.attention.self.key.weight', 'mlm.encoder.layer.5.attention.output.dense.bias', 'mlm.encoder.layer.8.attention.output.LayerNorm.weight', 'mlm.encoder.layer.7.output.dense.bias', 'mlm.encoder.layer.7.output.LayerNorm.bias', 'mlm.encoder.layer.3.attention.self.query.weight', 'mlm.encoder.layer.2.output.LayerNorm.bias', 'mlm.encoder.layer.5.output.dense.bias', 'mlm.encoder.layer.1.attention.self.query.bias', 'mlm.encoder.layer.1.attention.self.value.bias', 'mlm.encoder.layer.0.output.dense.weight', 'mlm.encoder.layer.1.intermediate.dense.bias', 'mlm.encoder.layer.5.attention.self.key.bias', 'mlm.encoder.layer.3.output.LayerNorm.weight', 'mlm.encoder.layer.2.attention.output.LayerNorm.bias', 'mlm.encoder.layer.6.attention.output.LayerNorm.weight', 'mlm.encoder.layer.8.output.LayerNorm.weight', 'mlm.encoder.layer.7.output.dense.weight', 'mlm.encoder.layer.1.attention.self.query.weight', 'mlm.encoder.layer.2.attention.self.value.bias', 'mlm.encoder.layer.9.output.dense.weight', 'mlm.encoder.layer.11.attention.output.LayerNorm.weight', 'mlm.encoder.layer.1.intermediate.dense.weight', 'mlm.encoder.layer.6.attention.self.query.weight', 'mlm.encoder.layer.6.output.LayerNorm.bias', 'mlm.encoder.layer.6.attention.self.value.weight', 'mlm.encoder.layer.9.attention.output.LayerNorm.weight', 'mlm.encoder.layer.4.attention.self.value.weight', 'mlm.encoder.layer.3.intermediate.dense.weight', 'mlm.pooler.dense.weight', 'mlm.encoder.layer.9.attention.output.dense.weight', 'mlm.encoder.layer.5.attention.self.key.weight', 'mlm.encoder.layer.0.intermediate.dense.bias', 'mlm.encoder.layer.0.attention.self.key.bias', 'mlm.encoder.layer.5.output.LayerNorm.bias', 'mlm.encoder.layer.7.attention.self.value.bias', 'mlm.encoder.layer.9.output.dense.bias', 'mlm.encoder.layer.9.intermediate.dense.weight', 'mlm.encoder.layer.3.attention.self.key.bias', 'mlm.encoder.layer.5.attention.output.LayerNorm.bias', 'mlm.encoder.layer.2.intermediate.dense.bias', 'mlm.encoder.layer.0.intermediate.dense.weight', 'mlm.encoder.layer.4.attention.output.dense.weight', 'mlm.encoder.layer.1.attention.output.dense.bias', 'mlm.embeddings.token_type_embeddings.weight', 'mlm.encoder.layer.7.attention.self.key.weight', 'mlm.encoder.layer.0.attention.output.LayerNorm.weight', 'mlm.encoder.layer.0.attention.output.dense.bias', 'mlm.encoder.layer.10.intermediate.dense.bias', 'mlm.encoder.layer.11.output.LayerNorm.weight', 'mlm.encoder.layer.10.attention.self.value.weight', 'mlm.encoder.layer.11.intermediate.dense.weight', 'mlm.encoder.layer.8.attention.self.key.bias', 'mlm.encoder.layer.8.output.LayerNorm.bias', 'mlm.encoder.layer.10.output.dense.bias', 'mlm.encoder.layer.6.output.dense.bias', 'mlm.encoder.layer.9.attention.self.value.weight', 'mlm.encoder.layer.0.attention.self.query.bias', 'mlm.encoder.layer.2.output.LayerNorm.weight', 'mlm.encoder.layer.6.attention.self.key.bias', 'mlm.encoder.layer.2.attention.self.query.weight', 'mlm.encoder.layer.8.attention.self.value.weight', 'mlm.encoder.layer.8.attention.self.query.weight', 'mlm.encoder.layer.4.attention.self.query.weight', 'mlm.encoder.layer.4.output.dense.weight', 'mlm.encoder.layer.0.attention.output.LayerNorm.bias', 'mlm.encoder.layer.10.attention.self.key.bias', 'mlm.encoder.layer.10.intermediate.dense.weight', 'mlm.encoder.layer.10.output.LayerNorm.weight', 'mlm.encoder.layer.4.attention.output.LayerNorm.weight', 'mlm.encoder.layer.9.output.LayerNorm.weight', 'mlm.encoder.layer.6.attention.self.key.weight', 'mlm.encoder.layer.7.attention.self.query.weight', 'mlm.encoder.layer.10.attention.self.query.weight', 'mlm.encoder.layer.2.output.dense.bias'], {}\n",
            "2024-05-03 15:14:46,197 - INFO - machamp.utils.myutils - Group 1: ['dataset_embedder.weight', 'decoders.slots.hidden_to_label.bias', 'decoders.intent.hidden_to_label.bias', 'decoders.slots.hidden_to_label.weight', 'decoders.intent.hidden_to_label.weight'], {}\n",
            "2024-05-03 15:14:46,197 - INFO - machamp.utils.myutils - Group 2: [], {}\n",
            "2024-05-03 15:14:46,197 - WARNING - machamp.utils.myutils - When constructing parameter groups, scalars.* does not match any parameter name\n",
            "2024-05-03 15:14:46,197 - INFO - machamp.utils.myutils - Number of trainable parameters: 177924956\n",
            "2024-05-03 15:14:46,206 - ERROR - STDERR - /usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "2024-05-03 15:14:46,206 - ERROR - STDERR -   warnings.warn(\n",
            "2024-05-03 15:14:47,184 - INFO - machamp.modules.allennlp.slanted_triangular - Gradual unfreezing. Training only the decoder heads, not the MLM.\n",
            "2024-05-03 15:14:47,186 - INFO - machamp.model.trainer - MaChAmp succesfully initialized in 56.0s\n",
            "2024-05-03 15:14:47,186 - INFO - machamp.model.trainer - \n",
            "\n",
            "          CHAMP!                   %%#/   \n",
            " ###/%(        CHAMP!            ,#//(%%#( \n",
            "#%%/((/(            . , ,       .%%#####%(\n",
            "#(/(((###(        ////////%     *//((((((/\n",
            "  ,((((//        #%(%#/#%%/          &####\n",
            "     ###### *#%%@&((//&@#%&(%%#%%%%#%%####\n",
            "     ,####(%%###%#&&&#((&&(((%%%%########(\n",
            "       ///((%%%##/%%######((%%%%%%%%((((//\n",
            "          %%%%%##((/%(///%%##%%%%%%%##    \n",
            "        /%%####%%%%%%#%%#/(%#(((%%%%%&.   \n",
            "       &#%/#%%###%%%##%%%((###(//(####(   \n",
            "     @&%%#////(########%######((/(////(   \n",
            "       #////     ((((##(((((////,         \n",
            "                   */(//(((/**,.          \n",
            "                  #,**%#%,*,,,*(          \n",
            "               /%%%%%/,,,,,(%%%%((        \n",
            "              &%%%#(((/*  /(#%%%#(/       \n",
            "              %%#(((//     ,((###(/       \n",
            "             .%(/(/,         /(###/(      \n",
            "              %(((//          (/((///     \n",
            "               ((/,             /(//      \n",
            "             %%%#((,            &%%%#     \n",
            "         /#(#%(/,             /###*##*    \n",
            "\n",
            "2024-05-03 15:14:47,186 - INFO - machamp.model.trainer - starting training...\n",
            "2024-05-03 15:14:47,187 - INFO - machamp.model.trainer - Epoch 1/1: training\n",
            "100% 1363/1363 [00:42<00:00, 31.75it/s]\n",
            "2024-05-03 15:15:30,117 - INFO - machamp.modules.allennlp.slanted_triangular - Learning rates for each group: \n",
            "\n",
            "2024-05-03 15:15:30,118 - INFO - machamp.modules.allennlp.slanted_triangular - 0: 3.125e-06\n",
            "2024-05-03 15:15:30,118 - INFO - machamp.modules.allennlp.slanted_triangular - 1: 3.125e-06\n",
            "2024-05-03 15:15:30,118 - INFO - machamp.modules.allennlp.slanted_triangular - 2: 1.1875e-06\n",
            "2024-05-03 15:15:30,118 - INFO - machamp.modules.allennlp.slanted_triangular - Gradual unfreezing finished. Training all layers.\n",
            "2024-05-03 15:15:30,118 - INFO - machamp.model.trainer - Epoch 1: evaluating on dev\n",
            "100% 10/10 [00:00<00:00, 29.51it/s]\n",
            "2024-05-03 15:15:30,459 - INFO - machamp.model.callback - epoch       : 1/1\n",
            "2024-05-03 15:15:30,459 - INFO - machamp.model.callback - best_epoch  : 1\n",
            "2024-05-03 15:15:30,459 - INFO - machamp.model.callback - max_gpu_mem : 1.1730\n",
            "2024-05-03 15:15:30,459 - INFO - machamp.model.callback - cur_ram     : 1.9091\n",
            "2024-05-03 15:15:30,459 - INFO - machamp.model.callback - time_epoch  : 0:00:43\n",
            "2024-05-03 15:15:30,459 - INFO - machamp.model.callback - time_total  : 0:00:43\n",
            "2024-05-03 15:15:30,459 - INFO - machamp.model.callback - \n",
            "2024-05-03 15:15:30,460 - INFO - machamp.model.callback -                 train_loss dev_loss train_scores dev_scores \n",
            "2024-05-03 15:15:30,460 - INFO - machamp.model.callback - Best (1)                                                    \n",
            "2024-05-03 15:15:30,460 - INFO - machamp.model.callback - intent_accuracy    63.7120  58.7517       0.4020     0.3067 \n",
            "2024-05-03 15:15:30,460 - INFO - machamp.model.callback - slots_accuracy      1.9538   1.4176       0.5253     0.5940 \n",
            "2024-05-03 15:15:30,460 - INFO - machamp.model.callback - sum                65.6658  60.1693       0.9273     0.9007 \n",
            "2024-05-03 15:15:30,460 - INFO - machamp.model.callback - Epoch 1                                                     \n",
            "2024-05-03 15:15:30,460 - INFO - machamp.model.callback - intent_accuracy    63.7120  58.7517       0.4020     0.3067 \n",
            "2024-05-03 15:15:30,460 - INFO - machamp.model.callback - slots_accuracy      1.9538   1.4176       0.5253     0.5940 \n",
            "2024-05-03 15:15:30,460 - INFO - machamp.model.callback - sum                65.6658  60.1693       0.9273     0.9007 \n",
            "2024-05-03 15:15:30,512 - INFO - machamp.model.callback - \n",
            "                Dev scores (y) over epochs (x)\n",
            "┌────────────────────────────────────────────────────────────┐\n",
            "││                             \u001b[35m▘\u001b[0m                             │ \n",
            "││                                                           │ \n",
            "││                                                           │ 0.55\n",
            "││                                                           │ \n",
            "││                                                           │ \n",
            "││                                                           │ 0.50\n",
            "││                                                           │ \n",
            "││                                                           │ \n",
            "││                                                           │ 0.45\n",
            "││                                                           │ \n",
            "││                                                           │ \n",
            "││                                                           │ 0.40\n",
            "││                                                           │ \n",
            "││                                                           │ \n",
            "││                                                           │ 0.35\n",
            "││                                                           │ \n",
            "││                             \u001b[34m▖\u001b[0m                             │ \n",
            "└────────────────────────────────────────────────────────────┘\n",
            "0.0            0.5            1.0            1.5\n",
            "                          \u001b[34m██\u001b[0m intent\n",
            "                          \u001b[35m██\u001b[0m slots\n",
            "2024-05-03 15:15:30,512 - INFO - machamp.model.callback - Performance of 0.9007 within top 1 models, saving to logs/nlu/2024.05.03_15.13.50/model_1.pt\n",
            "2024-05-03 15:15:40,305 - INFO - machamp.model.callback - Best performance obtained in epoch 1 linking model model_1.pt as logs/nlu/2024.05.03_15.13.50/model.pt.\n",
            "2024-05-03 15:15:40,305 - INFO - machamp.model.trainer - Predicting on dev set\n",
            "2024-05-03 15:15:41,438 - INFO - machamp.data.machamp_dataset - Reading data/xSID-0.3/en.valid.conll...\n",
            "2024-05-03 15:15:41,729 - INFO - machamp.readers.read_sequence - Stats NLU (data/xSID-0.3/en.valid.conll):\n",
            "2024-05-03 15:15:41,729 - INFO - machamp.readers.read_sequence - Lines:      300\n",
            "2024-05-03 15:15:41,729 - INFO - machamp.readers.read_sequence - Words:      2,377\n",
            "2024-05-03 15:15:41,729 - INFO - machamp.readers.read_sequence - Subwords:   3,091\n",
            "2024-05-03 15:15:41,729 - INFO - machamp.readers.read_sequence - Unks:       0\n",
            "2024-05-03 15:15:41,729 - INFO - machamp.readers.read_sequence - Pre-splits: 0\n",
            "2024-05-03 15:15:41,730 - INFO - machamp.data.machamp_dataset - Done reading data/xSID-0.3/en.valid.conll (0.0s)\n",
            "\n",
            "2024-05-03 15:15:41,730 - ERROR - STDERR - /usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
            "2024-05-03 15:15:41,730 - ERROR - STDERR -   warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n",
            "2024-05-03 15:15:42,276 - INFO - machamp.utils.myutils - intent_accuracy : 0.3067\n",
            "2024-05-03 15:15:42,276 - INFO - machamp.utils.myutils - slots_accuracy  : 0.5940\n",
            "2024-05-03 15:15:42,276 - INFO - machamp.utils.myutils - sum             : 0.9007\n",
            "2024-05-03 15:15:42,276 - INFO - machamp.utils.myutils - \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir predictions"
      ],
      "metadata": {
        "id": "xa3fhtStJP_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 predict.py logs/nlu/2024.05.03_15.13.50/model.pt data/xSID-0.3/en.test.conll predictions/nlu.xsid.out --device 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG-9w5D5I9b3",
        "outputId": "be35ce44-b00d-4386-b121-dee173e5ece0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-03 15:16:44.308474: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-03 15:16:44.308531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-03 15:16:44.309827: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-03 15:16:45.427608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-03 15:16:46,366 - INFO - __main__ - cmd: predict.py logs/nlu/2024.05.03_15.13.50/model.pt data/xSID-0.3/en.test.conll predictions/nlu.xsid.out --device 0\n",
            "\n",
            "2024-05-03 15:16:46,367 - INFO - __main__ - loading model...\n",
            "2024-05-03 15:16:47,265 - INFO - __main__ - predicting on data/xSID-0.3/en.test.conll, saving on predictions/nlu.xsid.out\n",
            "2024-05-03 15:16:47,798 - INFO - machamp.data.machamp_dataset - Reading data/xSID-0.3/en.test.conll...\n",
            "2024-05-03 15:16:48,463 - INFO - machamp.readers.read_sequence - Stats NLU (data/xSID-0.3/en.test.conll):\n",
            "2024-05-03 15:16:48,463 - INFO - machamp.readers.read_sequence - Lines:      500\n",
            "2024-05-03 15:16:48,464 - INFO - machamp.readers.read_sequence - Words:      3,903\n",
            "2024-05-03 15:16:48,464 - INFO - machamp.readers.read_sequence - Subwords:   5,012\n",
            "2024-05-03 15:16:48,464 - INFO - machamp.readers.read_sequence - Unks:       1\n",
            "2024-05-03 15:16:48,464 - INFO - machamp.readers.read_sequence - Pre-splits: 0\n",
            "2024-05-03 15:16:48,464 - INFO - machamp.data.machamp_dataset - Done reading data/xSID-0.3/en.test.conll (0.0s)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py:64: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
            "  warnings.warn(\"`data_source` argument is not used and will be removed in 2.2.0.\"\n",
            "2024-05-03 15:16:50,450 - INFO - machamp.utils.myutils - intent_accuracy : 0.3200\n",
            "2024-05-03 15:16:50,450 - INFO - machamp.utils.myutils - slots_accuracy  : 0.6080\n",
            "2024-05-03 15:16:50,450 - INFO - machamp.utils.myutils - sum             : 0.9280\n",
            "2024-05-03 15:16:50,450 - INFO - machamp.utils.myutils - \n",
            "\n"
          ]
        }
      ]
    }
  ]
}